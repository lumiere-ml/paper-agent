from datetime import datetime, timedelta
import pytz  # ç”¨äºå¤„ç†æ—¶åŒº
import io
import fitz  # PyMuPDF
import json
from openai import OpenAI
# from .const import *
from pdf2image import convert_from_path
import arxiv
PAPER_SUBJECT_PROMPT="Please determine whether the paper belongs to efficient LLM inference:\n The user will input the abstract of the paper when asking the question.\n\n Requirements:\n 1. Output in JSON format: {{'relevant': boolean, 'reason': string, 'prob': float}}, where 'prob' represents the confidence level, i.e., the probability you believe the paper aligns with the proposed subject.\n 2. Relevance criteria: The paper should involve performance optimization during the inference phase of large models.\n 3. Exclusions: Training optimization, hardware design, and non-performance-related research."
ARXIV_SUBJECTS = ["artificial intelligence", "Distributed, Parallel, and Cluster Computing", "Operating Systems"]
PAPER_READ_PROMPT= "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¡ç®—æœºæœºå™¨å­¦ä¹ é¢†åŸŸçš„è®ºæ–‡é˜…è¯»ä¸“å®¶ï¼Œç”¨æˆ·ä¼šè¾“å…¥ç»™ä½ è®ºæ–‡textå†…å®¹ï¼Œç„¶åä½ å¯¹å†…å®¹è¿›è¡Œé˜…è¯»åï¼Œå°†å¯¹è®ºæ–‡çš„é˜…è¯»ç»“æœæ•´ç†æˆå¦‚ä¸‹éƒ¨åˆ†ï¼Œè¦æ±‚ç”¨æˆ·èƒ½å¤Ÿåœ¨ä½ æ€»ç»“ä¸­èƒ½å¤Ÿé¢†ç•¥æ–‡ç« çš„å®—æ—¨å’Œæ ¸å¿ƒå†…å®¹,æœ€ç»ˆè¿”å›çš„ç»“æœæŒ‰ç…§å¦‚ä¸‹ç»“æ„ç»„ç»‡ï¼šæœ€ç»ˆç»“æœæŒ‰ç…§å¦‚ä¸‹ç»“æ„è¿›è¡Œæ•´ç†{{'titile':ä¸€å¥è¯æ€»ç»“æ­£ç¯‡æ–‡ç« ç‰¹ç‚¹ï¼Œå°çº¢ä¹¦æ–‡çŒ®é˜…è¯»æ ‡é¢˜é£æ ¼ï¼Œæˆ–è€…ç›´æ¥ç¿»è¯‘æ–‡ç« æ ‡é¢˜ï¼Œå‰ææ˜¯èƒ½è®©äººä¸€çœ¼çŸ¥é“æ–‡ç« æ˜¯å¹²å˜›çš„ã€‚'problem':ä¸»è¦è§£å†³çš„é—®é¢˜ï¼ˆçº¦200å­—ï¼Œæ ¸å¿ƒç‚¹åœ¨äºè¿™æ˜¯ä»€ä¹ˆé¢†åŸŸçš„é—®é¢˜ï¼Œå‘ç°äº†å‰äººå·¥ä½œçš„ä»€ä¹ˆç¼ºé™·ï¼Œé€šè¿‡ä»€ä¹ˆæ–¹æ³•è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼‰, 'insights':æ ¸å¿ƒè§‚å¯Ÿ/æ´è§ï¼ˆçº¦200å­—ï¼Œä¸€å®šè¦è®²æ¸…æ¥šæœ¬æ–‡åŸºäºå“ªäº›è§‚å¯Ÿï¼Œä¾é å“ªäº›å‘ç°å»è§£å†³é—®é¢˜ï¼‰,'main_method':é‡‡ç”¨çš„ä¸»è¦æ–¹æ³•ï¼ˆçº¦300å­—ï¼Œæ•´ç†ä¸ºpythonåˆ—è¡¨ç»“æ„,æ¯ä¸ªä¸»è¦æ–¹æ³•ä½œä¸ºå…¶ä¸­ä¸€ä¸ªå…ƒç´ ï¼Œä¾‹å¦‚[xxxx,xxx,...]ï¼‰,'gain':å®ç°çš„æ”¶ç›Šï¼ˆçº¦100å­—ï¼Œé‡åŒ–æŒ‡æ ‡ä¼˜å…ˆï¼‰}}"

WORK_SPACE="/home/mount_data/papers_read"

def fetch_recent_papers(subject, filter_key="inference", days=1):
    # è·å–UTCå½“å‰æ—¶é—´
    utc_now = datetime.now(pytz.utc)
    end_date = utc_now
    start_date = end_date - timedelta(days=days)

    # æ„é€ arXivæ”¯æŒçš„æ—¥æœŸèŒƒå›´æŸ¥è¯¢å­—ç¬¦ä¸²
    date_format = "%Y%m%d"
    query_date = (
        f"submittedDate:[{start_date.strftime(date_format)} "
        f"TO {end_date.strftime(date_format)}]"
    )

    # ç»„åˆå…³é”®è¯å’Œæ—¥æœŸæŸ¥è¯¢
    full_query = f"{subject} AND {query_date}"

    # åˆ›å»ºæœç´¢å¯¹è±¡ï¼ˆä¸é™åˆ¶ç»“æœæ•°é‡ï¼‰
    search = arxiv.Search(
        query=full_query,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending,  # æŒ‰æäº¤æ—¥æœŸé™åº
        max_results=float('inf')  # è®¾ç½®ä¸ºæ— é™å¤§ä»¥è·å–æ‰€æœ‰ç»“æœ
    )

    return [
        result for result in search.results()
        if result.summary  # ç¡®ä¿æ‘˜è¦å­˜åœ¨
           and filter_key in result.summary.lower()  # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
    ]


def is_the_paper_in_subject(llm_client, summary, prompt_str):
    completion = llm_client.chat.completions.create(
        model="hunyuan-turbo",
        messages=[
            {"role": "system",
             "content": prompt_str,
             },
            {
                "role": "user",
                "content": summary,
            },
        ],
        extra_body={
            "enable_enhancement": True,  # <- è‡ªå®šä¹‰å‚æ•°
        },
    )
    return completion.choices[0].message.content


def get_paper_content(paper):
    # ä¸‹è½½PDFå¹¶è§£æ
    response = requests.get(paper.pdf_url)
    doc = fitz.open(stream=io.BytesIO(response.content))
    text = ""
    for page in doc:
        text += page.get_text()
    return text


def sumarize_paper_content(llm_client, paper_content, system_prompt):
    completion = llm_client.chat.completions.create(
        model="hunyuan-turbo",
        messages=[
            {"role": "system",
             "content": system_prompt,
             },

            {
                "role": "user",
                "content": f"è¯·æŒ‰è¦æ±‚ï¼Œå¯¹è¿™ç¯‡æ–‡ç« è¿›è¡Œé˜…è¯»ï¼Œå¹¶è¾“å‡ºç»“æœ, å†…å®¹å¦‚ä¸‹{paper_content}"
            },
        ],
        extra_body={
            "enable_enhancement": False,  # <- è‡ªå®šä¹‰å‚æ•°
        },
    )
    return completion.choices[0].message.content


def emoji_xhs_template(data):
    """å¸¦Emojiçš„æ¨¡æ¿å¡«å……"""
    template = (
            "ğŸ—ã€" + data['title'] + "ã€‘\n\n"
                                   "ğŸ”¥ ç ”ç©¶ç—›ç‚¹\n" +
            data['problem'] + "\n\n"
                              "ğŸ’¡ æ ¸å¿ƒå‘ç°\n" +
            data['insights'] + "\n\n"
                               "ğŸš€ æŠ€æœ¯æ–¹æ¡ˆ\n" +
            '\n'.join([f'â–ªï¸{m}' for m in data['main_method']]) + "\n\n"
                                                                 "ğŸ“Š å®éªŒç»“æœ\n" +
            data['gain'] + "\n\n"
                           "#AIæŠ€æœ¯ #å¯æŒç»­å‘å±• #ç§‘ç ”å‰æ²¿"
    )
    return template


# æå–æ–‡ç« å›¾ç‰‡
def get_paper_images(paper_path, work_dir):
    """ å°†PDFæ‰€æœ‰é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½• """
    # è½¬æ¢å…¨éƒ¨é¡µé¢ï¼ˆé»˜è®¤ä¸æŒ‡å®šfirst_page/last_pageå‚æ•°ï¼‰
    images = convert_from_path(paper_path)

    saved_path = os.path.join(work_dir, 'images')
    os.makedirs(saved_path, exist_ok=True)
    for i, image in enumerate(images):
        # ç”Ÿæˆå¸¦é¡µç çš„æ–‡ä»¶åï¼Œå¦‚ï¼šfirst_page_1.png, first_page_2.png...
        filename = f"page_{i + 1}.png"  # é¡µç ä»1å¼€å§‹è®¡æ•°
        output_path = os.path.join(saved_path, filename)
        image.save(output_path)
    return


# åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹
import os
import re
import requests


def sanitize_folder_name(name):
    """
    æ¸…ç†æ–‡ä»¶å¤¹åç§°ä¸­çš„éæ³•å­—ç¬¦
    :param name: åŸå§‹æ–‡ä»¶å¤¹åç§°
    :return: æ¸…ç†åçš„æ–‡ä»¶å¤¹åç§°
    """
    # æ›¿æ¢éæ³•å­—ç¬¦ä¸ºä¸‹åˆ’çº¿
    sanitized_name = re.sub(r'[<>:"/\\|?*]', '_', name)
    # å»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    sanitized_name = sanitized_name.strip('. ')
    # é™åˆ¶é•¿åº¦ï¼ˆWindows è·¯å¾„æœ€å¤§é•¿åº¦ä¸º 255ï¼‰
    return sanitized_name[:200]


def create_folder_from_arxiv(work_space, title, arxiv_id):
    """
    æ ¹æ® arXiv ID åˆ›å»ºæ–‡ä»¶å¤¹
    :param arxiv_id: arXiv æ–‡ç« çš„ IDï¼ˆå¦‚ "2103.00001"ï¼‰
    """
    try:

        # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å¤¹å
        folder_name = sanitize_folder_name(title)

        # å°è¯•åˆ›å»ºæ–‡ä»¶å¤¹
        os.makedirs(os.path.join(work_space, folder_name), exist_ok=True)
        print(f"æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: {folder_name}")

    except Exception as e:
        print(f"ä½¿ç”¨è®ºæ–‡æ ‡é¢˜åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        print(f"å›é€€åˆ°ä½¿ç”¨ arXiv ID åˆ›å»ºæ–‡ä»¶å¤¹...")

        # ä½¿ç”¨ arXiv ID åˆ›å»ºæ–‡ä»¶å¤¹
        folder_name = f"arxiv_{arxiv_id}"
        os.makedirs(os.path.join(work_space, folder_name), exist_ok=True)
        print(f"æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: {folder_name}")
    return os.path.join(work_space, folder_name)


def main_work_flow(arxiv_subject=ARXIV_SUBJECTS, paper_subject_prompt=PAPER_SUBJECT_PROMPT,
                   paper_read_prompt=PAPER_READ_PROMPT,
                   filter_key="inference", days=3):
    # åˆæ­¥æŒ‰ç…§filteræŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆè¦æ±‚çš„è®ºæ–‡
    client = OpenAI(
        api_key="your api key",  # æ··å…ƒ APIKey
        base_url="https://api.hunyuan.cloud.tencent.com/v1",  # æ··å…ƒ endpoint
    )
    # åˆç­›papers
    paper_list = []
    for i in arxiv_subject:
        tmp_res = fetch_recent_papers(i, filter_key, days)
        paper_list.extend(tmp_res)
    # LLMè¾…åŠ©ç­›é€‰papers
    print("åˆç­›paperså®Œæˆ")
    papers_selected = []
    for paper_meta_info in paper_list:
        paper_select_info = is_the_paper_in_subject(client, paper_meta_info.summary, paper_subject_prompt)
        paper_select_info = json.loads(paper_select_info)
        paper_relevant = paper_select_info['relevant']
        paper_prob = paper_select_info['prob']
        print(paper_select_info)

        if paper_relevant and paper_prob >= 0.9:
            papers_selected.append(paper_meta_info)
    # é’ˆå¯¹æ¯ç¯‡paperè¿›è¡Œä¿¡æ¯æŠ½å–ä¸å¤§æ¨¡å‹é˜…è¯»ã€‚
    # å°†æ¯ç¯‡æ–‡ç« ä»ç„¶è¿›è¡Œä¸‹è½½å¹¶æå–å›¾ç‰‡ï¼Œå¹¶å°†åˆšæ‰çš„é˜…è¯»ç»“æœæ•´ç†æˆtxt
    print("å¤ç­›paperså®Œæˆ")
    for paper in papers_selected:
        paper_name = paper.title
        paper_id = paper.pdf_url.split('/')[-1]
        work_dir_path = create_folder_from_arxiv(WORK_SPACE, paper_name, paper_id)
        # ä¸‹è½½æ–‡ä»¶
        paper_path = paper.download_pdf(work_dir_path)
        source_path = paper.download_source(work_dir_path)
        # æå–å›¾ç‰‡
        get_paper_images(paper_path, work_dir_path)
        paper_context = get_paper_content(paper)
        llm_summarized = sumarize_paper_content(client, paper_context, paper_read_prompt)
        llm_summarized = json.loads(llm_summarized)
        llm_summ_xhs_style = emoji_xhs_template(llm_summarized)
        with open(os.path.join(work_dir_path, "summarized"), 'w', encoding='utf-8') as f:
            f.write(llm_summ_xhs_style)
    return


if __name__ == '__main__':
    main_work_flow()




